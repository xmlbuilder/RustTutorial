## 🧠 Rust에서 웹 스크래핑을 위한 주요 라이브러리
| 라이브러리 | 역할                                      | 특징 및 용도                                      |
|------------|-------------------------------------------|--------------------------------------------------|
| `reqwest`  | HTTP 요청 처리                            | GET/POST 요청, 헤더 설정, 쿠키, gzip 등 지원     |
| `scraper`  | HTML 파싱 및 DOM 탐색                     | CSS 셀렉터 기반 탐색, 자식/형제 요소 접근 가능   |
| `html5ever`| HTML5 파서                                | `scraper`의 기반, 저수준 DOM 처리 가능           |
| `select`   | HTML 문서 탐색용 셀렉터 엔진              | XPath 스타일 탐색 가능 (다소 저수준)             |
| `tokio`    | 비동기 런타임                             | `reqwest`와 함께 사용하여 비동기 스크래핑 구현   |


## 🧪 샘플 코드: HTML 파싱
```rust
use reqwest;
use scraper::{Html, Selector};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. HTML 다운로드
    let url = "https://www.euroncap.com/en/results/";
    let resp = reqwest::get(url).await?.text().await?;

    // 2. DOM 파싱
    let document = Html::parse_document(&resp);

    // 3. CSS 셀렉터 정의 (예: 차량 이름 추출)
    let car_selector = Selector::parse(".results-item-title").unwrap();

    // 4. 정보 추출
    for element in document.select(&car_selector) {
        let car_name = element.text().collect::<Vec<_>>().join(" ");
        println!("🚗 차량 이름: {}", car_name);
    }

    Ok(())
}
```

## 📦 Cargo.toml에 필요한 의존성
```
[dependencies]
reqwest = { version = "0.11", features = ["json", "blocking", "gzip", "brotli", "deflate", "cookies", "stream"] }
scraper = "0.13"
tokio = { version = "1", features = ["full"] }
```


## 🔧 확장 아이디어
- cron 또는 tokio-cron-scheduler로 주기적 실행
- ResultContainer에 상해치 저장
- DataSeries로 시각화
- serde_json으로 결과를 JSON으로 저장
- notify-rust로 알림 전송


# scraper 크레이트
Rust에서 HTML을 DOM처럼 탐색할 수 있게 해주는 도구.  
Sibling, Child, Parent 탐색도 가능하고, CSS 셀렉터 기반으로 아주 유연하게 사용할 수 있음.  

아래는 scraper를 사용해서:
- 특정 요소를 select하고
- 그 요소의 자식(child) 요소를 탐색하고
- 형제(sibling) 요소를 순회하는 예제야.

## 🧪 전체 샘플: Child + Sibling 탐색
```rust
use reqwest;
use scraper::{Html, Selector, ElementRef};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let url = "https://www.euroncap.com/en/results/";
    let html = reqwest::get(url).await?.text().await?;
    let document = Html::parse_document(&html);

    // 1. 차량 블록 선택 (예: 차량 정보 카드)
    let car_block_selector = Selector::parse(".results-item").unwrap();
    // 2. 자식 요소 셀렉터 (예: 차량 이름, 점수 등)
    let title_selector = Selector::parse(".results-item-title").unwrap();
    let score_selector = Selector::parse(".results-item-score").unwrap();

    for car_block in document.select(&car_block_selector) {
        // 자식 요소 접근
        if let Some(title) = car_block.select(&title_selector).next() {
            println!("🚗 차량 이름: {}", title.text().collect::<Vec<_>>().join(" "));
        }

        // 형제 요소 접근 (예: 점수)
        if let Some(score) = car_block.select(&score_selector).next() {
            println!("📊 점수: {}", score.text().collect::<Vec<_>>().join(" "));
        }

        println!("----------------------------------------");
    }

    Ok(())
}
```


## 🔍 핵심 개념 요약 (`scraper` 크레이트 기준)
| 동작                        | 설명                         | Rust API 사용법                                 |
|-----------------------------|------------------------------|-------------------------------------------------|
| `select()`                  | CSS 셀렉터로 요소 선택        | `document.select(&selector)`                    |
| `element.select()`          | 자식 요소 선택                | `element.select(&child_selector)`               |
| `element.next_sibling()`    | 형제 요소 접근                | `element.next_sibling()`                        |
| `element.text()`            | 텍스트 추출                   | `element.text().collect::<Vec<_>>()`            |



## 📦 필요한 크레이트
```
[dependencies]
reqwest = { version = "0.11", features = ["json", "blocking", "gzip", "brotli", "deflate", "cookies", "stream"] }
scraper = "0.13"
tokio = { version = "1", features = ["full"] }
```

## 🔚 결론
Rust에서도 Java처럼 API 기반으로 HTML을 파싱하고 정보를 추출하는 구조를 충분히 만들 수 있음.  
정기적으로 업데이트되는 데이터를 자동으로 수집하고 분석하는 시스템

