# SVD
## 🌟 SVD가 뭔지 초간단 직관으로 이해하기
## 🎯 SVD란?
- 어떤 아무 행렬이라도

```math
A=U\Sigma V^T
```

- 이렇게 세 개의 특별한 행렬로 쪼개는 방법.
  - U: 원래 데이터가 어떤 방향으로 퍼져 있는지 나타내는 회전(또는 방향)
  - Σ(시그마): 데이터가 각 방향으로 얼마나 크게 퍼져 있는지 나타내는 크기(스케일)
  - Vᵀ: 원래 좌표계를 어떤 방향으로 돌려야 하는지 알려주는 회전
- 즉, 회전 → 늘리기/줄이기 → 다시 회전
- 이 세 단계로 어떤 행렬이든 표현할 수 있다는 뜻.

## 🍕 비유로 이해하기
- 행렬 A를 피자를 만드는 과정이라고 생각해보자.
  - Vᵀ: 반죽을 특정 방향으로 돌려서 놓기
  - Σ: 반죽을 각 방향으로 다르게 늘리기(가로로 길게, 세로로 짧게 등)
  - U: 마지막으로 다시 돌려서 원하는 모양으로 만들기
- 이 세 단계만 있으면 어떤 모양의 피자든 만들 수 있음.
- 행렬도 마찬가지로, 어떤 변환이든 이 세 단계로 표현 가능.

## 🧠 왜 SVD가 등장했을까?
- ✔ 어떤 행렬이든 다 분해할 수 있는 가장 강력한 방법이 필요했기 때문
  - 역행렬이 없어도 됨
  - 정방행렬이 아니어도 됨
  - 대칭일 필요도 없음
- 모든 행렬에 적용 가능한 분해법이 바로 SVD.

## 📌 SVD는 어디에 쓰일까?
### 1️⃣ 차원 축소 (PCA)
- 데이터에서 가장 중요한 방향(주성분)을 찾는 데 사용
  - 이미지 압축, 데이터 노이즈 제거 등
### 2️⃣ 이미지 압축
- 이미지를 SVD로 분해하면
  - 큰 특이값만 남기고 작은 건 버려도 원본과 거의 비슷한 이미지가 나옴.
### 3️⃣ 추천 시스템 (넷플릭스, 유튜브)
- 사용자–아이템 행렬을 SVD로 분해
  - 숨겨진 취향 패턴을 찾아 추천
### 4️⃣ 선형 방정식의 최소제곱 해
- 해가 없거나 너무 많은 경우에도 가장 “그럴듯한” 해를 찾아줌
### 5️⃣ 신호 처리, 자연어 처리, 머신러닝 전반
- 데이터 구조를 가장 잘 설명하는 방향을 찾는 데 필수

## 🎉 한 줄 요약
- SVD는 어떤 행렬이든 ‘회전–늘리기–회전’으로 분해하는 방법이며, 데이터의 중요한 구조를 파악하거나 압축하는 데 매우 유용한 도구.

---

## 실용 적인 예
- 그림을 ‘말로 그려서’ SVD가 어떻게 생겼는지 아주 직관적으로 설명.
  - (정확한 그림 파일은 만들 수 없지만, 머릿속에 바로 떠오를 정도로 시각적으로 설명.)

### 🎨 1. 먼저, 원을 하나 그려봄
- 아래처럼 동그란 원이 있다고 상상.
```
     *****
   **     **
  *         *
  *         *
   **     **
     *****
```

- 이 원은 **기본 좌표계에서의 데이터** 라고 생각하면 됨.

## 🎨 2. 행렬 A가 이 원을 어떻게 바꾸는지 봄
- 행렬 A는 원을 타원으로 변형시키는 작업이라고 보면 됨.
- SVD는 이 변형을 3단계로 나눠서 설명.

### 🌀 단계 1: Vᵀ — 원을 먼저 ‘돌린다’
- (원) →  ↻  →  (기울어진 원)

- Vᵀ는 원을 어떤 각도로 돌려서
  - “데이터가 가장 잘 퍼지는 방향”과 일치시키는 역할을 함.

### 📏 단계 2: Σ — 가로·세로로 ‘늘리기/줄이기’
- 이제 기울어진 원을 가로로 늘리고, 세로로 줄이면 타원이 됨.
```
(기울어진 원)
      ↓ 늘리기/줄이기
(기울어진 타원)
```

- Σ(시그마) 안의 값들이
  - 얼마나 늘릴지
  - 얼마나 줄일지
결정하는 숫자.

## 🔄 단계 3: U — 타원을 다시 ‘돌려서’ 최종 위치로
- 마지막으로 타원을 다시 한 번 돌리면 행렬 A가 만든 최종 타원.
  - (기울어진 타원) → ↻ → (최종 타원)

## 🎯 전체 그림 요약
```
원
 ↓ Vᵀ (첫 번째 회전)
기울어진 원
 ↓ Σ (늘리기/줄이기)
기울어진 타원
 ↓ U (두 번째 회전)
최종 타원 = A가 만든 결과
```

- 즉, SVD는 어떤 행렬이든 “회전 → 늘리기 → 회전”의 조합으로 설명하는 방법.

## 🌟 왜 이게 직관적일까?
- 원은 대칭적이라 어떤 방향으로 돌려도 똑같음
- 그래서 **회전 → 늘리기 → 회전** 으로 모든 변형을 설명할 수 있음
- 이 과정에서 **데이터가 가장 많이 퍼진 방향(특이벡터)** 과 **그 퍼진 정도(특이값)** 가 자연스럽게 드러남

---
# Curve Fitting

## 🎯 전체 흐름 먼저 보기
- Curve fitting은 결국 데이터에 가장 잘 맞는 곡선(또는 직선)을 찾는 문제
- 수학적으로 풀면 선형 방정식의 최소제곱 문제가 됨.

- 그리고 최소제곱 문제를 가장 안정적이고 정확하게 푸는 방법이 SVD.

## 🧩 1단계: 데이터를 행렬로 만든다
- 예를 들어, 직선 $y=ax+b$ 을 데이터에 맞추고 싶다고 가정.
- 데이터가 이렇게 있다고 가정:
```
(1, 2)
(2, 3)
(3, 5)
(4, 4)
```

- 이걸 행렬로 만들면:
```
A = [ 1  1 ]
    [ 2  1 ]
    [ 3  1 ]
    [ 4  1 ]
```
```
x = [ a ]
    [ b ]
```
```
y = [ 2 ]
    [ 3 ]
    [ 5 ]
    [ 4 ]
```

- 우리가 원하는 건 $Ax\approx y$ 를 만족하는 a, b를 찾는 것.

## 📉 2단계: 하지만 Ax = y는 보통 정확히 풀리지 않는다
- 데이터가 완벽히 직선 위에 있지 않기 때문.
- 그래서 우리는 오차가 최소가 되는 x를 찾는 문제로 바꾼다.
- 즉,
```math
\| Ax-y\| ^2\mathrm{\  를\  최소화하는\  }x
```
- 이게 바로 최소제곱 문제.

## 🔍 3단계: 최소제곱 해를 구하는 가장 안정적인 방법 = SVD
- 여기서 SVD가 등장한다.
- 행렬 A를 SVD로 분해하면:
```math
A=U\Sigma V^T
```
- 이걸 이용하면 최소제곱 해는
```math
x=V\Sigma ^+U^Ty
```
여기서 $\Sigma ^+$ 는 시그마의 역수만 뒤집어놓은 것(의사역행렬).

## 🎨 4단계: 그림으로 직관적으로 이해하기
### (1) 데이터 점들
```
   *
 *    *
      * 
    *
```

### (2) 이 점들을 가장 잘 통과하는 직선을 찾고 싶다
```
   *
 *    *
      * 
    *
---------
```

### (3) 이 문제를 행렬 A로 표현하면, A는 **기저 방향** 을 가진다
- SVD는 A를
  - 회전(Vᵀ)
  - 늘리기/줄이기(Σ)
  - 다시 회전(U)
- 로 분해한다.
### (4) 이 과정에서 A가 가진 “중요한 방향”이 드러난다
- 데이터가 가장 많이 퍼진 방향
- 덜 퍼진 방향
- 이 두 방향이 직선의 기울기와 절편을 결정하는 핵심 정보가 된다.
### (5) 그래서 SVD는 데이터의 구조를 가장 잘 반영한 해를 준다
- 특히:
  - 데이터가 노이즈가 많을 때
  - A가 거의 선형종속일 때
  - 수치적으로 불안정할 때
- SVD는 가장 안정적인 해를 제공한다.

## 🧠 왜 SVD를 쓰는가? (Curve fitting 관점)

| 이유                      | 설명 |
|---------------------------|-------------------------------------------------------------|
| 수치적으로 가장 안정적    | AᵀA를 직접 계산하면 오차가 커지지만, SVD는 안정적으로 해를 구할 수 있다. |
| 노이즈에 강함             | 작은 특이값을 무시하면 자연스럽게 노이즈가 제거되어 더 부드러운 곡선이 나온다. |
| 해가 없거나 너무 많아도 해결 가능 | SVD는 의사역행렬(pseudoinverse)을 통해 항상 해를 만들 수 있어 under/over-determined 문제 모두 해결된다. |
| 모든 행렬에 적용 가능     | 정방행렬이 아니어도, 심지어 랭크가 부족해도 SVD는 적용된다. |
| 데이터 구조를 잘 반영     | 특이값과 특이벡터가 데이터가 퍼진 방향을 정확히 잡아주어 가장 의미 있는 fitting 결과를 준다. |

## 🎉 한 줄 요약
Curve fitting에서 SVD는 **데이터에 가장 잘 맞는 곡선을 찾기 위한 최소제곱 해** 를 가장 안정적이고 정확하게 구하는 핵심 도구다.

-----

# 전체 요약

## 1. SVD란 무엇인가
### 1.1 정의
- 어떤 행렬 A가 있을 때, SVD(Singular Value Decomposition)는 다음과 같은 분해입니다.
  - $A=U\Sigma V^T$
  - $A$ : 임의의 $m\times n$ 행렬 (정방행렬일 필요 없음)
  - $U$ : $m\times m$ 직교 행렬 (열벡터: 왼쪽 특이벡터)
  - $\Sigma : m\times n$ 대각(또는 직사각) 행렬, 대각원소는 특이값 $\sigma _i\geq 0$
  - $V$ : $n\times n$ 직교 행렬 (열벡터: 오른쪽 특이벡터)
- 핵심 요약:
  - $V^T$: 좌표계를 “좋은 방향”으로 회전시키는 변환
  - $\Sigma$ : 각 방향으로 늘리기/줄이기(스케일 변화)
  - $U$ : 다시 회전시켜 최종 좌표계로 보내는 변환
- 즉, SVD는 임의의 선형변환 A를 `회전 → 늘리기/줄이기 → 회전` 세 단계로 쪼갠 표현입니다.


### 1.2 원이 타원으로 변하는 기하적 직관
- 2차원에서 단위 원(모든 방향에서 길이가 1인 점들의 집합)을 생각해 봅시다.
- 처음 상태: 단위 원
```
     *****
   **     **
  *         *
  *         *
   **     **
     *****
```
- $V^T$ 적용: 좌표계 회전 → 원은 여전히 원
- 회전은 거리와 각도를 보존하므로, 모양은 변하지 않고 방향만 바뀜
- $\Sigma$  적용: 각 축 방향으로 다른 비율로 늘리기/줄이기
- 원이 타원으로 변하는 핵심 단계
  - 예: 가로 3배, 세로 1배 등
- U 적용: 다시 회전 → 타원의 방향이 바뀜
  - 모양은 그대로, 방향만 바뀜
- 정리하면:
  - 형태(원→타원) 변화의 핵심은 $\Sigma$ 
  - **$U$, $V^T$** 는 회전만 담당 (모양은 유지, 방향만 변경)

## 2. $\Sigma$ 를 바꾸면 무엇이 달라지는가
### 2.1 $A=U\Sigma V^T$ 의 의미
- 중요한 점:
```math
A=U\Sigma V^T
```

- 이 식은 그냥 A를 정확하게 표현한 분해입니다.
  - $U$, $\Sigma$ , $V^T$ 를 그대로 곱하면 다시 원래의 A가 됩니다.
  - **분해했다** 는 것은 A를 바꿨다는 뜻이 아니라,
  - A를 세 개의 **의미 있는 변환** 으로 나눠서 본다는 뜻입니다.
### 2.2 $\Sigma$ 를 바꾸면?
- $\Sigma$ 를 로 바꾸면 가 되며, 이것은 원래 A가 아닌 새로운 선형변환입니다.
- 이때:
- $U$, $V^T$: 여전히 회전(방향 정보)
- $\Sigma$ : 늘리기/줄이기 강도를 바꾸므로, 원이 타원으로 변하는 정도, 타원의 납작함 등 형태 변화가 달라집니다.
- 즉,- **$U,V^T$**는 건드리면 A의 **방향 특성** 이 변해버립니다.
- **$\Sigma$** 를 조정하면 A의 **크기/자유도** 만 조절하면서 새로운 변환 A'를 만들 수 있습니다.

## 3. Curve fitting 문제의 기본 형태
### 3.1 문제 설정예를 들어 직선 y=ax+b를 데이터에 fitting한다고 합시다.
- 데이터 $(x_i,y_i)$ 가 $i=1,\dots ,n$ 개 있을 때:
```math
  A=\left[ \begin{matrix}x_1&1\\ x_2&1\\ \vdots &\vdots \\ x_n&1\end{matrix}\right] ,\quad x=\left[ \begin{matrix}a\\ b\end{matrix}\right] ,\quad y=\left[ \begin{matrix}y_1\\ y_2\\ \vdots \\ y_n\end{matrix}\right]
```
- 우리가 원하는 것은:
  - $Ax\approx y$ 를 만족하는
```math
x=\left[ \begin{matrix}a\\ b\end{matrix}\right]
```
- 입니다.
### 3.2 최소제곱 문제로의 변환데이터는 보통 직선 위에 정확히 있지 않으므로:
$Ax=y$ 를 정확히 만족하는 $x$ 는 존재하지 않을 수 있습니다.
- 그래서 우리는 오차를 최소화합니다.
  - $\min _x\| Ax-y\| ^2$ 이것이 최소제곱(least squares) 문제입니다.

## 4. 고전적인 해법과 그 한계
### 4.1 Normal equation고전적으로는 다음 공식을 사용합니다.
$x=(A^TA)^{-1}A^Ty$ 하지만 이 방식은 다음과 같은 문제를 가집니다.
- 문제 1: $A^TA$ 가 역행렬을 가질 만큼 잘 conditioned 되어 있어야 함
- 문제 2: A의 열이 서로 비슷하면 $A^TA$ 는 거의 `singular`
- 문제 3: 수치적으로 매우 불안정 (floating point 오차 증폭)
- 그래서 실전에서는 더 안정적인 방법이 필요합니다.
- 거기서 SVD가 등장합니다.

## 5. SVD를 사용한 최소제곱 해 구하기
### 5.1 SVD 분해먼저 A를 SVD로 분해합니다.
- $A=U\Sigma V^T$ 여기서 $\Sigma$ 는 대각 성분에 특이값 $\sigma _i$ 를 가진 행렬입니다.
### 5.2 의사역행렬(pseudoinverse)을 이용한 해SVD를 이용하면 최소제곱 해는 다음과 같이 쓸 수 있습니다.
- $x=V\Sigma ^+U^Ty- \Sigma ^+: \Sigma$ 의 의사역행렬
- $\sigma _i\neq 0$ 인 경우, $\frac{1}{\sigma _i}$ 로 바꿔서 대각에 배치
- 0인 특이값에 대해서는 0으로 둠
- 이 식은 normal equation을 직접 풀지 않고도 최소제곱 해를 구할 수 있게 합니다.
## 6. 이 과정을 기하적으로 해석하기
- $x=V\Sigma ^+U^Ty$ 이 식을 단계별 기하적 변환으로 보면:
- $U^Ty$
  - 역할: y를 A가 “표현할 수 있는 방향”으로 투영
  - 해석: A가 만들어내는 공간(열공간/이미지)에 y를 가져다 놓는 과정
- $\Sigma ^+$
  - 역할: 각 특이값 방향으로 나누기(스케일 조정)
  - 해석:
  - 특이값이 큰 방향: 정보가 많은 방향 → 그대로 잘 사용
  - 특이값이 작은 방향: 노이즈 방향 → 과도하게 확대하지 않도록 제어
- $V$
  - 역할: 변환된 계수들을 다시 원래 변수 공간으로 회전
  - 해석: 단순화된 좌표계에서 계산된 해를 원래 좌표계로 돌려놓음
## 7. 자유도 감소의 의미 (SVD 관점)
### 7.1 특이값과 자유도
```math
\Sigma =\mathrm{diag}(\sigma _1,\sigma _2,\dots ,\sigma _r)
```
- 큰 $\sigma _i$: 데이터가 많이 퍼져 있는 중요한 방향
- 작은 $\sigma _i$: 데이터가 거의 없는 방향, 노이즈 방향
- 여기서 자유도 감소란:
  - 작은 특이값을 0으로 만들거나 무시하는 것
- 이때:
  - 랭크가 감소 → 사용되는 독립된 방향(자유도)이 줄어듦
  - 모델이 더 단순해지고 안정적이 됨
### 7.2 **삼차원 곡면이 UV 평면이 되는 느낌** 
- 데이터가 3차원 공간에 있지만 실제로는 거의 어떤 2차원 평면(또는 1차원 선) 근처에 몰려 있다고 합시다.
- V^T: 3D 공간을 회전시켜
  - 데이터가 가장 퍼져 있는 방향들을 축과 정렬
  - 이때, 실제 정보가 있는 방향은 2개, 나머지 1개는 거의 정보 없음
- $\Sigma$ : 이 세 축의 중요도를 특이값으로 표현
  - $\sigma _1,\sigma _2$: 큼 (실제 평면)
  - $\sigma _3$: 매우 작음 (평면에서 조금 벗어난 노이즈)
  - $\sigma _3$ 를 0으로 만든다 → 자유도 1 감소
  - 3D에서 2D 평면으로 **눕혀지는** 느낌
- 곡면이 아니라 **실제로 의미 있는 평면(uv plane)** 만 남는 것
  - Curve fitting에서는 **데이터가 실제로 차지하는 유효한 차원만 사용하고, 나머지는 버려서 과적합과 불안정을 막는 과정** 이라고 이해하면 됩니다.

## 8. Curve fitting에서 SVD를 쓰는 이유 정리
- 아래는 Curve fitting 관점에서 SVD를 쓰는 이유를 요약.

| 이유                          | 설명 |
|-------------------------------|------|
| 수치적으로 안정적             | \(A^T A\)를 직접 사용하지 않으므로, ill-conditioned 문제에서도 해를 보다 안정적으로 구할 수 있다. |
| 노이즈에 강함                 | 작은 특이값(노이즈 방향)을 잘라내거나 완화함으로써 더 부드러운 fitting 결과를 얻을 수 있다. |
| 자유도(랭크) 조절 가능        | 특이값을 선택적으로 사용하여 모델의 복잡도를 줄이고, 실제 데이터가 있는 차원만 사용한다. |
| 해가 없거나 너무 많은 경우 해결 | under-determined, over-determined 시스템 모두에서 의사역행렬을 통해 의미 있는 해를 구할 수 있다. |
| 데이터 구조를 잘 반영         | 특이벡터가 데이터가 퍼진 방향을 잘 포착하여, 가장 의미 있는 방향을 기준으로 fitting을 수행한다. |

## 9. 전체 흐름 한 번에 요약
- SVD 분해
```math 
A=U\Sigma V^T
```
- Curve fitting 문제
```math
\min _x\| Ax-y\| ^2
```
- SVD를 이용한 최소제곱 해
```math
x=V\Sigma ^+U^Ty
```
- 기하적 해석
- $V^T$: 좌표계 회전 → 데이터가 잘 퍼진 방향으로 정렬
- $\Sigma$ : 각 방향의 중요도(특이값) → 필요하면 작은 특이값 제거(자유도 감소)
- $U^T$: y를 A가 표현 가능한 공간으로 투영
- $\Sigma ^+, V$ : 단순화된 좌표계에서 계산 후, 원래 공간으로 복귀
- 자유도 감소
  - 작은 특이값을 0 또는 무시
  - 실제 데이터가 놓인 **유효 차원** 만 사용
  - 3D 곡면을 회전시켜 UV 평면 위에 눕히고, 필요 없는 축은 버리는 느낌

## 마무리

- SVD는 **회전(U, V^T)** + **스케일 변화(Σ)** 로 모든 선형변환을 표현한다.
- Curve fitting에서는 SVD를 사용하여
  - 수치적으로 안정적인 최소제곱 해를 구하고,
  - 특이값을 통해 **자유도(랭크)를 조절** 하며,
  - 실제 데이터가 존재하는 차원만 사용해 **과적합과 노이즈**를 줄일 수 있다.
- **삼차원 곡면을 회전시켜 UV 평면으로 눕힌 뒤, 필요 없는 축은 버리는 느낌** 이라는 직관은  
  SVD와 자유도 감소를 이해하는 데 매우 정확한 비유이다.

---
