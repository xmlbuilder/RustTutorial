# Transfer Learning
Transfer Learning(전이 학습)은 한 작업에서 학습한 지식을 다른 관련 작업에 재사용하여 성능을 향상시키는 머신러닝 기법입니다.  
즉, 이미 학습된 모델을 새로운 문제에 맞게 조정(fine-tuning)하거나 특징 추출(feature extraction) 도구로 활용하는 방식.

## 📌 기본 개념
- 전통적 학습: 새로운 문제마다 처음부터 모델을 학습해야 함 → 많은 데이터와 계산 자원 필요.
- Transfer Learning: 기존에 학습된 모델(예: 이미지 분류, 언어 모델)을 가져와 새로운 문제에 맞게 재사용 → 데이터와 자원 절약.
- 핵심 아이디어: 한 도메인에서 학습한 **표현(representation)** 을 다른 도메인에서도 활용할 수 있다는 점.

## 📊 주요 전략
### 1. 	Feature Extraction (특징 추출)
- 기존 모델의 대부분 레이어를 고정(freeze)하고, 마지막 분류기만 새 데이터에 맞게 학습.
- 예: ResNet을 가져와 새로운 이미지 데이터셋에 맞게 마지막 레이어만 학습.
### 2. 	Fine-Tuning (미세 조정)
- 기존 모델의 일부 레이어를 계속 학습시켜 새로운 데이터에 적응.
- 예: BERT를 가져와 특정 도메인 텍스트(법률, 의료)에 맞게 추가 학습.

## 🎯 활용 분야
- 컴퓨터 비전: ImageNet으로 학습된 CNN을 가져와 의료 영상, 위성 이미지 분석에 활용.
- 자연어 처리(NLP): GPT, BERT 같은 대규모 언어 모델을 가져와 감성 분석, 질의응답, 번역에 적용.
- 음성 인식: 일반 음성 데이터로 학습된 모델을 특정 방언이나 환경에 맞게 조정.
- 저자원 환경: 데이터가 부족한 경우, 대규모 데이터로 학습된 모델을 기반으로 빠르게 성능 확보.

## 📌 장점
- 데이터 절약: 적은 양의 데이터로도 좋은 성능 확보 가능.
- 시간/비용 절감: 처음부터 학습할 필요가 없어 학습 속도 빠름.
- 범용성: 다양한 도메인에 쉽게 적용 가능.

## 📌 요약
- 👉 Transfer Learning은 이미 학습된 모델을 새로운 문제에 맞게 재사용하는 기법으로, 데이터 부족 문제를 해결하고 학습 효율을 크게 높여줍니다.
- 👉 특히 컴퓨터 비전과 NLP 분야에서 필수적인 기술로 자리 잡고 있습니다

---
